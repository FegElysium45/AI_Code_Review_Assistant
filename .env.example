# LLM Provider Configuration
# Uncomment and set the provider you want to use

# OpenAI
OPENAI_API_KEY=sk-your-key-here

# Anthropic
# ANTHROPIC_API_KEY=sk-ant-your-key-here

# Local LLM (e.g., Ollama, vLLM)
# LOCAL_LLM_ENDPOINT=http://localhost:8000/v1/chat/completions

# Default provider (openai, anthropic, local)
LLM_PROVIDER=openai

# Default model
LLM_MODEL=gpt-4
```

---

### **4. .gitignore**
```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
env/
ENV/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# Environment variables
.env

# Output
output/
*.log

# Testing
.pytest_cache/
.coverage
htmlcov/

# OS
.DS_Store
Thumbs.db